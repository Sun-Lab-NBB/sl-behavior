from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count

from numba import njit, prange  # type: ignore
import numpy as np
import polars as pl
from numpy.typing import NDArray
from ataraxis_base_utilities import console, chunk_iterable
from sl_shared_assets import SessionData, ProcessingTracker, SessionTypes, AcquisitionSystems, TrackerFileNames
from sl_shared_assets.data_classes import SessionLock


def _process_frame_message_batch(arguments: tuple[Path, list[str], np.uint64]) -> list[np.uint64]:
    """Processes the target batch of VideoSystem-generated messages stored in the .npz log file.

    This worker function is used by the _extract_camera_timestamps() function to process multiple message batches in
    parallel to speed up the overall camera timestamp data processing.

    Args:
        arguments: A tuple of three elements (log_path, file_names, onset_us). The first element stores the path to the
            processed log file. The second element stores the names of the individual message .npy files stored in the
            target archive. The third element stores the onset of the frame data acquisition, in microseconds elapsed
            since UTC epoch onset.

    Returns:
        The list of frame acquisition timestamps for all frames whose messages have been processed as part of the
        batch, stored as microseconds since UTC epoch onset.
    """

    # Unpacks the data from the input tuple
    log_path, file_names, onset_us = arguments

    # Opens the processed log archive using memory mapping. If frame processing is performed in parallel, all processes
    # interact with the archive concurrently.
    with np.load(log_path, allow_pickle=False, fix_imports=False, mmap_mode='r') as archive:
        frame_timestamps = []

        # Loops over the batch of frame messages and extracts frame acquisition timestamps.
        for item in file_names:
            message = archive[item]

            # Frame timestamp messages do not have a payload, they only contain the source ID and the acquisition
            # timestamp. This gives them the length of 9 bytes.
            if len(message) == 9:

                # Extracts the number of microseconds elapsed since acquisition onset and uses it to calculate the
                # global timestamp for the message, in microseconds since UTC epoch onset.
                elapsed_microseconds = message[1:9].view(np.uint64).item()
                frame_timestamps.append(onset_us + elapsed_microseconds)

    return frame_timestamps


def _extract_camera_timestamps(
        log_path: Path,
        n_workers: int = -1,
) -> tuple[np.uint64, ...]:
    """Extracts the video camera frame acquisition timestamps from the target .npz log file generated by a VideoSystem
    instance during sl-experiment runtime.

    This worker function was copied from the ataraxis-video-system library and optimized to use multiprocessing to
    achieve a measurable speed while processing large log files. Primarily, this was done to decouple the sl-behavior
    library from ataraxis-video-system to support using newer Python versions.

    Notes:
        If the target .npz archive contains fewer than 2000 messages, the processing is carried out sequentially
        regardless of the specified worker-count.

    Args:
        log_path: The path to the .npz log file that stores the logged data generated by the VideoSystem
            instance during runtime.
        n_workers: The number of parallel worker processes (CPU cores) to use for processing. Setting this to a value
        below 1 uses all available CPU cores. Setting this to a value of 1 conducts the processing sequentially.

    Returns:
        A tuple that stores the frame acquisition timestamps. Each timestamp is stored as the number of microseconds
        since the UTC epoch onset.

    Raises:
        ValueError: If the target .npz archive does not exist.
    """
    # Ensures that the target .npz log archive exists.
    if not log_path.exists() or log_path.suffix != ".npz" or not log_path.is_file():
        error_message = (
            f"Unable to extract camera frame timestamp data from the log file {log_path}, as it does not exist or does "
            f"not point to a valid .npz archive."
        )
        console.error(message=error_message, error=ValueError)

    # Memory-maps the processed archive to conserve RAM. The first processing pass is designed to find the onset
    # timestamp value.
    with np.load(log_path, allow_pickle=False, fix_imports=False, mmap_mode='r') as archive:

        # Locates the logging onset timestamp. The onset is used to convert the relative timestamps for logged frame
        # data into absolute UTC timestamps. Originally, all timestamps other than onset are stored as elapsed time in
        # microseconds relative to the onset timestamp.
        onset_us = np.uint64(0)
        timestamp_offset = 0
        message_list = list(archive.files)
        for number, item in enumerate(message_list):
            message: NDArray[np.uint8] = archive[item]  # Extracts message payload from the compressed .npy file

            # Recovers the uint64 timestamp value from each message. The timestamp occupies 8 bytes of each logged
            # message starting at index 1. If the timestamp value is 0, the message contains the onset timestamp value
            # stored as an 8-byte payload. Index 0 stores the source ID (uint8 value).
            timestamp_value = message[1:9].view(np.uint64).item()
            if timestamp_value == 0:

                # Extracts the byte-serialized UTC timestamp stored as microseconds since epoch onset.
                onset_us = np.uint64(message[9:].view(np.int64).item())

                # Breaks the loop once the onset is found. Generally, the onset is expected to be found very early into
                # the loop.
                timestamp_offset = number  # Records the item number at which the onset value was found.
                break

    # Builds the list of files to process after discovering the timestamp (the list of remaining messages)
    messages_to_process = message_list[timestamp_offset + 1:]

    # If there are no leftover messages to process, return an empty tuple
    if not messages_to_process:
        return tuple()

    # Small archives are processed sequentially to avoid the unnecessary overhead of setting up the multiprocessing
    # runtime. This is also done for large files if the user explicitly requests to use a single worker process.
    if n_workers == 1 or len(messages_to_process) < 2000:
        return tuple(_process_frame_message_batch((log_path, messages_to_process, onset_us)))

    # If the user enabled using all available cores, configures the runtime to use all available CPUs
    if n_workers < 0:
        n_workers = cpu_count()

    # Creates batches of messages to process during runtime. Uses a fairly high batch multiplier to create many smaller
    # batches, which leads to a measurable increase in the processing speed, especially for large archives. The optimal
    # multiplier value (4) was determined experimentally.
    batches = []
    batch_indices = []  # Keeps track of batch order
    for i, batch in enumerate(chunk_iterable(messages_to_process, n_workers*4)):
        if batch:
            batches.append((log_path, batch, onset_us))
            batch_indices.append(i)

    # Processes batches using ProcessPoolExecutor
    with ProcessPoolExecutor(max_workers=n_workers) as executor:

        # Submits all tasks
        future_to_index = {
            executor.submit(_process_frame_message_batch, batch): idx
            for idx, batch in zip(batch_indices, batches)
        }

        # Collects results while maintaining frame order. This also propagates processing errors to the caller process.
        results = [None] * len(batches)
        completed = 0
        for future in as_completed(future_to_index):
            results[future_to_index[future]] = future.result()
            completed += 1

    # Combines processing results in order
    all_timestamps = []
    for batch_timestamps in results:
        # noinspection PyUnreachableCode
        if batch_timestamps is not None:  # Skips None results
            all_timestamps.extend(batch_timestamps)

    return tuple(all_timestamps)


def process_camera_timestamps(session: SessionData, log_id: int, manager_id: int) -> None:
    """Reads the specified camera log .npz file and extracts the frame acquisition timestamps as a .feather file.

    Args:
        session: The initialized SessionData instance for the session for which to process the camera frame timestamps.
        log_id: The name (ID) of the log archive to process. Message log archives are named after the ID of the
            source that produces the logged messages at runtime.
    """

    # Resolves the path to the output.feather file
    log_path = session.source_data.behavior_data_path.joinpath(f"{log_id}_log.npz")

    if session.acquisition_system == AcquisitionSystems.MESOSCOPE_VR:
        if log_id == 51:
            output_path = session.source_data.camera_data_path.joinpath("face_camera_timestamps.feather")
        elif log_id == 62:
            output_path = session.source_data.camera_data_path.joinpath("left_camera_timestamps.feather")
        elif log_id == 73:
            output_path = session.source_data.camera_data_path.joinpath("right_camera_timestamps.feather")
        else:
            raise ValueError()
    else:
        raise NotImplementedError()

    # Ensures that the manager process for this runtime has exclusive access to the processed session's data
    lock = SessionLock(file_path=session.tracking_data.session_lock_path)
    lock.acquire(manager_id=manager_id)

    # Ensures that the behavior processing runtime is currently ongoing.
    tracker = ProcessingTracker(file_path=session.tracking_data.tracking_data_path.joinpath(TrackerFileNames.BEHAVIOR))
    tracker.start(manager_id=manager_id)
    try:

        # Extracts timestamp data from log archive
        timestamp_data = _extract_camera_timestamps(log_path)

        # Converts extracted data to Polars series.
        timestamps_series = pl.Series(name="frame_time_us", values=timestamp_data)

        # Saves extracted data using Feather format and no compression to support memory-mapping the file during
        # processing.
        timestamps_series.to_frame().write_ipc(file=output_path, compression="uncompressed")
    finally:
        tracker.stop(manager_id=manager_id)
